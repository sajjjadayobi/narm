# Course Baseline
  - Stanford 2021 as [baseline](http://web.stanford.edu/class/cs224n/)
  - Transformers
    - Tokenizers:
      - byte pair encdoing , ...
    - positinal encoding
    - all type of normalization
  - Bert
    - LMs (AR, Masked)
    - Contextual Embeddings
    - next sentence prediction
    - AdamW
    - gule activation function
    - Weight Decay
    - Word Piece
    - CLS token
  - Practice
    - Keras Examples
      - text clasification with a single transformers
      - train Bert from scrtch
      - semantic similarity with Transformers
      - Question Answering with Transforers
    - Hugging Face
      - pipeline
      - how use
        - TFTrainer (in 20 lines of code)
      - fine-tune 
        - text classification
        - with a custom dataset
  - Roberta
  - Albert
  - GPT 2 & 3
  - T5
  
  - OPTINALS
    - Multilingual Transformers
    - distil-Bert
    - Fast-Training
    - LongFormer
    - Question Answering
    - Lamb optimization
    
  - Impilimentations
    - HuggingFace Examples
  
    
# Rules:
  - Bublish
    - Course interview 
      - with marckdown
    - and other stuff on my blog posts
  - for any topics
    - see original paper
    - search for paper explaner
    - see other blog post
    - and search it on Youtube


# [Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z): Natural Language Processing with Deep Learning
- Transformers
- Bert
- My Notes

# [Yanick](https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew): 
- Transformers
- Bert
- GPT-2-3
- Roberta
- T5
- LayerNorm

# [Lecun Course](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq)
- Transformers

# [jalammar](http://jalammar.github.io/)
- Transformers
- Bert
- Language Models interpruter
- GPT-2-3

# [dive to ai](https://d2l.ai)
- Transformers
- Bert (with training)

# Single
- [Albert](https://amitness.com/2020/02/albert-visual-summary) from chaudhary
- [Transfromers](lena-voita.github.io) lena


